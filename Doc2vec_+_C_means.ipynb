{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYWtTEKhSCUe",
        "outputId": "e7b14c71-dd80-41ef-81e7-da3ece8069b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sent2vec\n",
            "  Downloading sent2vec-0.3.0-py3-none-any.whl (8.1 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from sent2vec) (1.11.0+cu113)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from sent2vec) (2.2.4)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 21.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from sent2vec) (3.6.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sent2vec) (1.21.6)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from sent2vec) (3.6.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim->sent2vec) (1.4.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim->sent2vec) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->sent2vec) (6.0.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->sent2vec) (8.13.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->sent2vec) (1.4.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->sent2vec) (0.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->sent2vec) (21.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->sent2vec) (1.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->sent2vec) (57.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy->sent2vec) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->sent2vec) (3.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->sent2vec) (0.9.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy->sent2vec) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->sent2vec) (2.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->sent2vec) (1.0.7)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->sent2vec) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->sent2vec) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->sent2vec) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy->sent2vec) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy->sent2vec) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->sent2vec) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy->sent2vec) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy->sent2vec) (4.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->sent2vec) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->sent2vec) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->sent2vec) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->sent2vec) (3.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->sent2vec) (3.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->sent2vec) (2019.12.20)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 57.3 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 57.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers->sent2vec) (21.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers->sent2vec) (3.0.9)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers, sent2vec\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 sent2vec-0.3.0 tokenizers-0.12.1 transformers-4.19.2\n"
          ]
        }
      ],
      "source": [
        "!pip install sent2vec"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import gensim\n",
        "#import contractions\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from sklearn.cluster import KMeans\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn import cluster\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.manifold import TSNE\n",
        "from nltk.cluster import KMeansClusterer, euclidean_distance\n",
        "import nltk\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.cluster import DBSCAN\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "#import flair\n",
        "from scipy.spatial import distance\n",
        "#from simpletransformers.language_representation import RepresentationModel\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "#import umap.umap_ as umap\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.metrics import silhouette_score\n",
        "from wordcloud import WordCloud\n",
        "#import texthero as hero\n",
        "from sklearn import metrics\n",
        "from scipy import spatial\n",
        "from sent2vec.vectorizer import Vectorizer\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "7FcZe0VCTKgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers\n",
        "#!pip install \"gensim==3.8.1\"\n",
        "#!pip install texthero\n",
        "!pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAv5U6OvTKjV",
        "outputId": "6f797df5-02eb-44e2-bcc0-738f1be7f50b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.0.tar.gz (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.19.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.12.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 75.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (4.2.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.7.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.11.4)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.8.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.0-py3-none-any.whl size=120747 sha256=2349a83627a50846d5599e1435df451f9b56ec89da146d9a92d869964768c925\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/c0/df/b6873ab7aac3f2465aa9144b6b4c41c4391cfecc027c8b07e7\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentencepiece, sentence-transformers\n",
            "Successfully installed sentence-transformers-2.2.0 sentencepiece-0.1.96\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.72-py2.py3-none-any.whl (8.3 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting anyascii\n",
            "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 27.8 MB/s \n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 55.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.1 contractions-0.1.72 pyahocorasick-1.4.4 textsearch-0.0.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dO93LYCETKmB",
        "outputId": "24f54352-7842-48b1-8949-4dfdd4971b83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train=pd.read_csv('/content/drive/MyDrive/BBC News Train.csv')\n",
        "data_train.drop(['ArticleId'],inplace=True,axis=1)\n",
        "data_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "0LtyuR0nTKon",
        "outputId": "043b5763-7d57-489e-e995-d6daa996babb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                Text  Category\n",
              "0  worldcom ex-boss launches defence lawyers defe...  business\n",
              "1  german business confidence slides german busin...  business\n",
              "2  bbc poll indicates economic gloom citizens in ...  business\n",
              "3  lifestyle  governs mobile choice  faster  bett...      tech\n",
              "4  enron bosses in $168m payout eighteen former e...  business"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0c5f94d3-44d4-4aba-8d66-a92e839a52ef\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>worldcom ex-boss launches defence lawyers defe...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>german business confidence slides german busin...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>bbc poll indicates economic gloom citizens in ...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>lifestyle  governs mobile choice  faster  bett...</td>\n",
              "      <td>tech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>enron bosses in $168m payout eighteen former e...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0c5f94d3-44d4-4aba-8d66-a92e839a52ef')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0c5f94d3-44d4-4aba-8d66-a92e839a52ef button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0c5f94d3-44d4-4aba-8d66-a92e839a52ef');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_text=data_train['Text'].to_list()"
      ],
      "metadata": {
        "id": "pl2SSciMTKrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tagged_document(list_of_list_of_words):\n",
        "  for i,list_of_words in enumerate(list_of_list_of_words):\n",
        "    yield gensim.models.doc2vec.TaggedDocument(list_of_words,[i])\n",
        "\n",
        "data_for_training=list(tagged_document(list_text))      "
      ],
      "metadata": {
        "id": "eJ05XKBdTKuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=gensim.models.doc2vec.Doc2Vec(vector_size=2240)"
      ],
      "metadata": {
        "id": "exgCWiw6TKxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.build_vocab(data_for_training)"
      ],
      "metadata": {
        "id": "_jm9pORSTK1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Features from Doc2Vec"
      ],
      "metadata": {
        "id": "k56W1xeZVFzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features=[]\n",
        "for i in range(len(list_text)):\n",
        "  features.append(model.infer_vector(list_text[i].split()))"
      ],
      "metadata": {
        "id": "NzuY9RHlTK3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features=np.array(features)\n",
        "type(features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEtxCpT_ZQoc",
        "outputId": "e099eb2b-8062-4b67-a9f6-88515abc10fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzy-c-means"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWY9SqejYYM5",
        "outputId": "37a42225-5caa-413d-bb91-816686f9c9f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fuzzy-c-means\n",
            "  Downloading fuzzy_c_means-1.6.3-py3-none-any.whl (9.1 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from fuzzy-c-means) (1.21.6)\n",
            "Collecting typer<0.4.0,>=0.3.2\n",
            "  Downloading typer-0.3.2-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: tabulate<0.9.0,>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from fuzzy-c-means) (0.8.9)\n",
            "Collecting pydantic<2.0.0,>=1.8.2\n",
            "  Downloading pydantic-1.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.1 MB 46.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pydantic<2.0.0,>=1.8.2->fuzzy-c-means) (4.2.0)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.2->fuzzy-c-means) (7.1.2)\n",
            "Installing collected packages: typer, pydantic, fuzzy-c-means\n",
            "Successfully installed fuzzy-c-means-1.6.3 pydantic-1.9.1 typer-0.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fcmeans import FCM\n",
        "my_model = FCM(n_clusters=5,random_state=42) \n",
        "my_model.fit(features)\n",
        "data_train['cluster']=my_model.predict(features)"
      ],
      "metadata": {
        "id": "P_nLnRfhTK6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_to_category = {}\n",
        "for cat in data_train['Category'].unique():\n",
        "    mark = data_train['Category'] == cat\n",
        "    top = data_train[mark]['cluster'].value_counts().head(1)\n",
        "    count = top.values[0]\n",
        "    cluster = top.index[0]\n",
        "    print(f\"{cat}:\\n Top cluster number: {cluster}, Number of samples: {count}\")\n",
        "    cluster_to_category[cluster] = cat\n",
        "\n",
        "print(\"\\nMap cluster number to category:\")\n",
        "cluster_to_category"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhCh-COUTK8O",
        "outputId": "3a0f37fb-e4db-4097-ad09-97dc65303d75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "business:\n",
            " Top cluster number: 0, Number of samples: 88\n",
            "tech:\n",
            " Top cluster number: 0, Number of samples: 64\n",
            "politics:\n",
            " Top cluster number: 4, Number of samples: 69\n",
            "sport:\n",
            " Top cluster number: 0, Number of samples: 81\n",
            "entertainment:\n",
            " Top cluster number: 2, Number of samples: 80\n",
            "\n",
            "Map cluster number to category:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'sport', 2: 'entertainment', 4: 'politics'}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train['clustered_category'] = data_train['cluster'].map(cluster_to_category)"
      ],
      "metadata": {
        "id": "EtIpb80LTK_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Overall accuracy of clustered categories:', np.mean(data_train['Category'] == data_train['clustered_category']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAbhEeEATLCA",
        "outputId": "b12ba632-b457-4d5c-a29b-e1b367b76be6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall accuracy of clustered categories: 0.15436241610738255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "print(\"Silhouette Coefficient: %0.3f\"% metrics.silhouette_score(features,data_train['cluster']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAdi52aKFugr",
        "outputId": "a68ce33e-e0d1-4262-d822-9a6c8159a909"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette Coefficient: 0.001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Doc2vec + Preporcessed Text + C-Means"
      ],
      "metadata": {
        "id": "rwbEOWZGkqls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import gensim\n",
        "import contractions\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from sklearn.cluster import KMeans\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn import cluster\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from nltk.cluster import KMeansClusterer, euclidean_distance\n",
        "import nltk\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.cluster import DBSCAN\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "ulWn2aNwTLEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords') \n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "data_train=pd.read_csv('/content/drive/MyDrive/BBC News Train.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXuL8oC3TLHJ",
        "outputId": "420cdf4f-dd97-4c3d-8a6e-dba8c1d4baac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "STOPWORDS = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "xAnLn9nsTLKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train.drop(['ArticleId'],axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "xn0pd85ETLO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def expand_contractions(text):\n",
        "    expanded_words = []    \n",
        "    for word in text.split():\n",
        "        expanded_words.append(contractions.fix(word))   \n",
        "    expanded_text = ' '.join(expanded_words)\n",
        "    return expanded_text\n",
        " \n",
        "def remove_stopwords(text):\n",
        "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
        "def remove_digits(text):\n",
        "    return \" \".join([word for word in str(text).split() if not(word.isdigit())])\n",
        "def remove_shorttokens(text):\n",
        "    return \" \".join([word for word in str(text).split() if len(word)>1])\n",
        "def lemmatize_words(text):\n",
        "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "#convert it to string\n",
        "data_train[\"Text\"]=data_train[\"Text\"].astype(str)\n",
        "#Convert to lower and strip\n",
        "data_train[\"Text\"]=data_train[\"Text\"].str.lower().str.strip()\n",
        "#apply contractions\n",
        "data_train[\"Text\"]=data_train[\"Text\"].apply(lambda x:expand_contractions(x))\n",
        "#remove punctuations\n",
        "data_train[\"Text\"]=data_train[\"Text\"].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '' , x))\n",
        "#remove stop words\n",
        "data_train[\"Text\"]=data_train[\"Text\"].apply(lambda x: remove_stopwords(x))\n",
        "#Lemmatize the sentence\n",
        "data_train[\"Text\"]=data_train[\"Text\"].apply(lambda text: lemmatize_words(text))\n",
        "data_train[\"Text\"]=data_train[\"Text\"].apply(lambda text:remove_digits(text))\n",
        "data_train[\"Text\"]=data_train[\"Text\"].apply(lambda text:remove_shorttokens(text))"
      ],
      "metadata": {
        "id": "alYjVRy9TLR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_text=data_train['Text'].to_list()"
      ],
      "metadata": {
        "id": "5V57J1UVTLTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tagged_document(list_of_list_of_words):\n",
        "  for i,list_of_words in enumerate(list_of_list_of_words):\n",
        "    yield gensim.models.doc2vec.TaggedDocument(list_of_words,[i])\n",
        "\n",
        "data_for_training=list(tagged_document(list_text))    "
      ],
      "metadata": {
        "id": "haL_rJ8RTLWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1=gensim.models.doc2vec.Doc2Vec(vector_size=1600)\n",
        "model1.build_vocab(data_for_training)"
      ],
      "metadata": {
        "id": "IX29ODDVTLYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features=[]\n",
        "for i in range(len(list_text)):\n",
        "  features.append(model1.infer_vector(list_text[i].split()))"
      ],
      "metadata": {
        "id": "dxi36QzRTLbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features=np.array(features)"
      ],
      "metadata": {
        "id": "SdUyCcBKTLeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fcmeans import FCM\n",
        "my_model = FCM(n_clusters=5,random_state=42) \n",
        "my_model.fit(features)\n",
        "data_train['cluster']=my_model.predict(features)"
      ],
      "metadata": {
        "id": "31-5YFNLTLhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_to_category = {}\n",
        "for cat in data_train['Category'].unique():\n",
        "    mark = data_train['Category'] == cat\n",
        "    top = data_train[mark]['cluster'].value_counts().head(1)\n",
        "    count = top.values[0]\n",
        "    cluster = top.index[0]\n",
        "    print(f\"{cat}:\\n Top cluster number: {cluster}, Number of samples: {count}\")\n",
        "    cluster_to_category[cluster] = cat\n",
        "\n",
        "print(\"\\nMap cluster number to category:\")\n",
        "cluster_to_category"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGSbsm3gTLkO",
        "outputId": "2f6ecf3a-1fb7-4f9a-c3b2-f746cbe3a445"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "business:\n",
            " Top cluster number: 0, Number of samples: 81\n",
            "tech:\n",
            " Top cluster number: 1, Number of samples: 60\n",
            "politics:\n",
            " Top cluster number: 1, Number of samples: 74\n",
            "sport:\n",
            " Top cluster number: 0, Number of samples: 81\n",
            "entertainment:\n",
            " Top cluster number: 4, Number of samples: 65\n",
            "\n",
            "Map cluster number to category:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'sport', 1: 'politics', 4: 'entertainment'}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train['clustered_category'] = data_train['cluster'].map(cluster_to_category)\n",
        "print('Overall accuracy of clustered categories:', np.mean(data_train['Category'] == data_train['clustered_category']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_PlX8TlTLpe",
        "outputId": "2d24443c-b1a6-44f8-f3fc-e30da93ca78a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall accuracy of clustered categories: 0.1476510067114094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "print(\"Silhouette Coefficient: %0.3f\"% metrics.silhouette_score(features,data_train['cluster']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aangePFGI0u",
        "outputId": "db74ad2b-b235-4f0b-ae3f-94a6ae7daefc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette Coefficient: 0.001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Experiment with PCA"
      ],
      "metadata": {
        "id": "SQj-HebQoHtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_embbedding=features\n",
        "x=[]\n",
        "y=[]\n",
        "for i in range(1490,0,-20):\n",
        "  pca_2 = PCA(n_components=i)\n",
        "  pca_2_result = pca_2.fit_transform(data_embbedding)\n",
        "  x.append(i)\n",
        "  y.append(round(np.sum(pca_2.explained_variance_ratio_),2))\n",
        "  print('Cumulative variance explained by {} principal components: {:.2%}'.format(i,np.sum(pca_2.explained_variance_ratio_)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77OiObPATLtz",
        "outputId": "8c3d4fcc-05de-48cf-b643-0fda5a663d38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cumulative variance explained by 1490 principal components: 100.00%\n",
            "Cumulative variance explained by 1470 principal components: 100.00%\n",
            "Cumulative variance explained by 1450 principal components: 100.00%\n",
            "Cumulative variance explained by 1430 principal components: 100.00%\n",
            "Cumulative variance explained by 1410 principal components: 99.99%\n",
            "Cumulative variance explained by 1390 principal components: 99.97%\n",
            "Cumulative variance explained by 1370 principal components: 99.95%\n",
            "Cumulative variance explained by 1350 principal components: 99.92%\n",
            "Cumulative variance explained by 1330 principal components: 99.88%\n",
            "Cumulative variance explained by 1310 principal components: 99.83%\n",
            "Cumulative variance explained by 1290 principal components: 99.77%\n",
            "Cumulative variance explained by 1270 principal components: 99.69%\n",
            "Cumulative variance explained by 1250 principal components: 99.61%\n",
            "Cumulative variance explained by 1230 principal components: 99.50%\n",
            "Cumulative variance explained by 1210 principal components: 99.38%\n",
            "Cumulative variance explained by 1190 principal components: 99.23%\n",
            "Cumulative variance explained by 1170 principal components: 99.07%\n",
            "Cumulative variance explained by 1150 principal components: 98.90%\n",
            "Cumulative variance explained by 1130 principal components: 98.69%\n",
            "Cumulative variance explained by 1110 principal components: 98.47%\n",
            "Cumulative variance explained by 1090 principal components: 98.22%\n",
            "Cumulative variance explained by 1070 principal components: 97.94%\n",
            "Cumulative variance explained by 1050 principal components: 97.65%\n",
            "Cumulative variance explained by 1030 principal components: 97.31%\n",
            "Cumulative variance explained by 1010 principal components: 96.96%\n",
            "Cumulative variance explained by 990 principal components: 96.57%\n",
            "Cumulative variance explained by 970 principal components: 96.15%\n",
            "Cumulative variance explained by 950 principal components: 95.70%\n",
            "Cumulative variance explained by 930 principal components: 95.21%\n",
            "Cumulative variance explained by 910 principal components: 94.69%\n",
            "Cumulative variance explained by 890 principal components: 94.13%\n",
            "Cumulative variance explained by 870 principal components: 93.54%\n",
            "Cumulative variance explained by 850 principal components: 92.89%\n",
            "Cumulative variance explained by 830 principal components: 92.20%\n",
            "Cumulative variance explained by 810 principal components: 91.48%\n",
            "Cumulative variance explained by 790 principal components: 90.70%\n",
            "Cumulative variance explained by 770 principal components: 89.90%\n",
            "Cumulative variance explained by 750 principal components: 89.03%\n",
            "Cumulative variance explained by 730 principal components: 88.12%\n",
            "Cumulative variance explained by 710 principal components: 87.15%\n",
            "Cumulative variance explained by 690 principal components: 86.13%\n",
            "Cumulative variance explained by 670 principal components: 85.06%\n",
            "Cumulative variance explained by 650 principal components: 83.93%\n",
            "Cumulative variance explained by 630 principal components: 82.74%\n",
            "Cumulative variance explained by 610 principal components: 81.50%\n",
            "Cumulative variance explained by 590 principal components: 80.19%\n",
            "Cumulative variance explained by 570 principal components: 78.81%\n",
            "Cumulative variance explained by 550 principal components: 77.37%\n",
            "Cumulative variance explained by 530 principal components: 75.87%\n",
            "Cumulative variance explained by 510 principal components: 74.27%\n",
            "Cumulative variance explained by 490 principal components: 72.61%\n",
            "Cumulative variance explained by 470 principal components: 70.88%\n",
            "Cumulative variance explained by 450 principal components: 69.08%\n",
            "Cumulative variance explained by 430 principal components: 67.18%\n",
            "Cumulative variance explained by 410 principal components: 65.23%\n",
            "Cumulative variance explained by 390 principal components: 63.17%\n",
            "Cumulative variance explained by 370 principal components: 61.04%\n",
            "Cumulative variance explained by 350 principal components: 58.79%\n",
            "Cumulative variance explained by 330 principal components: 56.47%\n",
            "Cumulative variance explained by 310 principal components: 54.05%\n",
            "Cumulative variance explained by 290 principal components: 51.51%\n",
            "Cumulative variance explained by 270 principal components: 48.90%\n",
            "Cumulative variance explained by 250 principal components: 46.17%\n",
            "Cumulative variance explained by 230 principal components: 43.30%\n",
            "Cumulative variance explained by 210 principal components: 40.36%\n",
            "Cumulative variance explained by 190 principal components: 37.26%\n",
            "Cumulative variance explained by 170 principal components: 34.01%\n",
            "Cumulative variance explained by 150 principal components: 30.68%\n",
            "Cumulative variance explained by 130 principal components: 27.89%\n",
            "Cumulative variance explained by 110 principal components: 24.19%\n",
            "Cumulative variance explained by 90 principal components: 20.36%\n",
            "Cumulative variance explained by 70 principal components: 16.28%\n",
            "Cumulative variance explained by 50 principal components: 12.02%\n",
            "Cumulative variance explained by 30 principal components: 7.48%\n",
            "Cumulative variance explained by 10 principal components: 2.62%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Experiment with PCA-->95%,90%,85% covarience--920,780,670\n",
        "data_embbedding=features\n",
        "pca_95 = PCA(n_components=925)\n",
        "pca_95result = pca_95.fit_transform(data_embbedding)\n",
        "\n",
        "pca_90 = PCA(n_components=780)\n",
        "pca_90result = pca_95.fit_transform(data_embbedding)\n",
        "\n",
        "pca_85 = PCA(n_components=670)\n",
        "pca_85result = pca_85.fit_transform(data_embbedding)"
      ],
      "metadata": {
        "id": "CifeMoYgTLxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fcmeans import FCM\n",
        "my_model95= FCM(n_clusters=5,random_state=42) \n",
        "my_model95.fit(pca_95result)\n",
        "data_train['cluster95']=my_model95.predict(pca_95result)\n",
        "\n",
        "my_model90= FCM(n_clusters=5,random_state=42) \n",
        "my_model90.fit(pca_90result)\n",
        "data_train['cluster90']=my_model95.predict(pca_90result)\n",
        "\n",
        "my_model85= FCM(n_clusters=5,random_state=42) \n",
        "my_model85.fit(pca_85result)\n",
        "data_train['cluster85']=my_model85.predict(pca_85result)"
      ],
      "metadata": {
        "id": "z8PU1jwUTL1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_to_category95= {}\n",
        "for cat in data_train['Category'].unique():\n",
        "    mark = data_train['Category'] == cat\n",
        "    top = data_train[mark]['cluster95'].value_counts().head(1)\n",
        "    count = top.values[0]\n",
        "    cluster = top.index[0]\n",
        "    print(f\"{cat}:\\n Top cluster number: {cluster}, Number of samples: {count}\")\n",
        "    cluster_to_category95[cluster] = cat\n",
        "\n",
        "print(\"\\nMap cluster number to category:\")\n",
        "cluster_to_category95"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hy0WFSasTL4G",
        "outputId": "d24e8aba-4962-44b6-ab3d-d706f856f8ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "business:\n",
            " Top cluster number: 2, Number of samples: 81\n",
            "tech:\n",
            " Top cluster number: 1, Number of samples: 61\n",
            "politics:\n",
            " Top cluster number: 1, Number of samples: 72\n",
            "sport:\n",
            " Top cluster number: 0, Number of samples: 81\n",
            "entertainment:\n",
            " Top cluster number: 4, Number of samples: 64\n",
            "\n",
            "Map cluster number to category:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'sport', 1: 'politics', 2: 'business', 4: 'entertainment'}"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train['clustered_category95'] = data_train['cluster95'].map(cluster_to_category95)\n",
        "print('Overall accuracy of clustered categories:', np.mean(data_train['Category'] == data_train['clustered_category95']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qb124AzKTL_C",
        "outputId": "e25ede66-779d-4416-c13d-4a835cd6731a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall accuracy of clustered categories: 0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "print(\"Silhouette Coefficient: %0.3f\"% metrics.silhouette_score(pca_95result,data_train['cluster95']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xy9ZT7fI9d1",
        "outputId": "13b03c33-8127-4575-843e-6028a1b307f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette Coefficient: 0.001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_to_category90= {}\n",
        "for cat in data_train['Category'].unique():\n",
        "    mark = data_train['Category'] == cat\n",
        "    top = data_train[mark]['cluster90'].value_counts().head(1)\n",
        "    count = top.values[0]\n",
        "    cluster = top.index[0]\n",
        "    print(f\"{cat}:\\n Top cluster number: {cluster}, Number of samples: {count}\")\n",
        "    cluster_to_category90[cluster] = cat\n",
        "\n",
        "print(\"\\nMap cluster number to category:\")\n",
        "cluster_to_category90"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUDyGZnSTMBU",
        "outputId": "ed5bbc14-09b4-467c-8c69-447456ae1786"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "business:\n",
            " Top cluster number: 2, Number of samples: 80\n",
            "tech:\n",
            " Top cluster number: 1, Number of samples: 61\n",
            "politics:\n",
            " Top cluster number: 1, Number of samples: 72\n",
            "sport:\n",
            " Top cluster number: 0, Number of samples: 80\n",
            "entertainment:\n",
            " Top cluster number: 4, Number of samples: 64\n",
            "\n",
            "Map cluster number to category:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'sport', 1: 'politics', 2: 'business', 4: 'entertainment'}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train['clustered_category90'] = data_train['cluster90'].map(cluster_to_category90)\n",
        "print('Overall accuracy of clustered categories:', np.mean(data_train['Category'] == data_train['clustered_category90']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buMvdSM3TMCp",
        "outputId": "7c57511a-5591-4596-86dd-0f925578211c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall accuracy of clustered categories: 0.19865771812080538\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "print(\"Silhouette Coefficient: %0.3f\"% metrics.silhouette_score(pca_90result,data_train['cluster90']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGnE3LK6JCo6",
        "outputId": "f66834f4-db78-4581-8b89-19899071716b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette Coefficient: 0.001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_to_category85= {}\n",
        "for cat in data_train['Category'].unique():\n",
        "    mark = data_train['Category'] == cat\n",
        "    top = data_train[mark]['cluster85'].value_counts().head(1)\n",
        "    count = top.values[0]\n",
        "    cluster = top.index[0]\n",
        "    print(f\"{cat}:\\n Top cluster number: {cluster}, Number of samples: {count}\")\n",
        "    cluster_to_category85[cluster] = cat\n",
        "\n",
        "print(\"\\nMap cluster number to category:\")\n",
        "cluster_to_category85"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PmCtdZNTMFC",
        "outputId": "5ab4cf7e-f9e9-4b3e-ba91-d6bf3b16ba44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "business:\n",
            " Top cluster number: 0, Number of samples: 83\n",
            "tech:\n",
            " Top cluster number: 1, Number of samples: 63\n",
            "politics:\n",
            " Top cluster number: 1, Number of samples: 71\n",
            "sport:\n",
            " Top cluster number: 0, Number of samples: 80\n",
            "entertainment:\n",
            " Top cluster number: 4, Number of samples: 62\n",
            "\n",
            "Map cluster number to category:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'sport', 1: 'politics', 4: 'entertainment'}"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train['clustered_category85'] = data_train['cluster85'].map(cluster_to_category90)\n",
        "print('Overall accuracy of clustered categories:', np.mean(data_train['Category'] == data_train['clustered_category85']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5taRe3rYTMHM",
        "outputId": "c097cafc-94f9-4abe-fd35-1dbb4292a743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall accuracy of clustered categories: 0.1966442953020134\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "print(\"Silhouette Coefficient: %0.3f\"% metrics.silhouette_score(pca_85result,data_train['cluster85']))"
      ],
      "metadata": {
        "id": "rzK2Z4YSTMJN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "097dbaad-ef95-4837-c2b2-50ec2f398d0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette Coefficient: 0.000\n"
          ]
        }
      ]
    }
  ]
}